"""
Training script for EventTransformerClassifier.

This script loads pre-processed tensor data and trains the medical event
classifier with support for:
- Per-event predictions
- Feature missingness handling
- Cumulative procedure context
- Multi-label classification

Usage:
    python train.py --data_path processed_data.json --epochs 10 --batch_size 32
"""

import argparse
import json
import torch
import torch.nn as nn
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from pathlib import Path
from typing import Dict, List
import random
from tqdm import tqdm

from model.data_loader import create_dataloader
from model.event_classifier import EventTransformerClassifier


def load_tensor_data(data_path: str) -> Dict[str, Dict]:
    """
    Load tensor data from JSON file.
    
    Expected format:
    {
        'patient_id': {
            'events': List[List[float]],  # Will be converted to tensors
            'procedures': List[List[List[float]]],  # Per-event procedures
            'targets': List[List[float]]  # Per-event targets
        }
    }
    
    Args:
        data_path: Path to JSON file containing tensor data
        
    Returns:
        Dictionary mapping patient_id -> {'events': List[Tensor], 'procedures': List[List[Tensor]], 'targets': List[Tensor]}
    """
    print(f"Loading data from {data_path}...")
    
    with open(data_path, 'r') as f:
        raw_data = json.load(f)
    
    # Convert lists to tensors
    tensor_data = {}
    for patient_id, patient_data in raw_data.items():
        # Convert events
        events = [torch.tensor(e, dtype=torch.float32) for e in patient_data['events']]
        
        # Convert procedures (List[List[Tensor]])
        procedures = []
        for event_procs in patient_data['procedures']:
            event_proc_tensors = [torch.tensor(p, dtype=torch.float32) for p in event_procs]
            procedures.append(event_proc_tensors)
        
        # Convert targets
        targets = [torch.tensor(t, dtype=torch.float32) for t in patient_data['targets']]
        
        tensor_data[patient_id] = {
            'events': events,
            'procedures': procedures,
            'targets': targets
        }
    
    print(f"✓ Loaded data for {len(tensor_data)} patients")
    return tensor_data


def split_data(tensor_data: Dict, train_ratio: float = 0.8, val_ratio: float = 0.1):
    """
    Split data into train/val/test sets.
    
    Args:
        tensor_data: Dictionary of patient data
        train_ratio: Proportion for training
        val_ratio: Proportion for validation
        
    Returns:
        Tuple of (train_data, val_data, test_data)
    """
    patient_ids = list(tensor_data.keys())
    random.shuffle(patient_ids)
    
    n = len(patient_ids)
    n_train = int(n * train_ratio)
    n_val = int(n * val_ratio)
    
    train_ids = patient_ids[:n_train]
    val_ids = patient_ids[n_train:n_train + n_val]
    test_ids = patient_ids[n_train + n_val:]
    
    train_data = {pid: tensor_data[pid] for pid in train_ids}
    val_data = {pid: tensor_data[pid] for pid in val_ids}
    test_data = {pid: tensor_data[pid] for pid in test_ids}
    
    print(f"Data split: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")
    return train_data, val_data, test_data


def get_text_feature_ranges(data_dir: str = None, schema_type: str = 'event') -> dict:
    """
    Load text feature ranges from JSON file generated by datamodel.py.
    
    The ranges are automatically detected during Event.to_tensor() conversion
    and exported to text_ranges.json. This function loads them for use in training.
    
    Args:
        data_dir: Directory containing text_ranges.json. If None, uses default.
        schema_type: Which schema to load ranges for ('event' or 'procedure')
    
    Returns:
        Dict mapping feature names to (start_idx, end_idx) tuples
        
    Example return:
        {
            'diagnosis': (50, 434),  # Diagnosis text embedded at indices 50-433
            'procedure_desc': (434, 818),  # Procedure description at 434-817
        }
    """
    if data_dir is None:
        # Default to data/ directory relative to this script
        data_dir = Path(__file__).parent / 'data'
    else:
        data_dir = Path(data_dir)
    
    text_ranges_path = data_dir / 'text_ranges.json'
    
    if not text_ranges_path.exists():
        print(f"Warning: Text ranges file not found at {text_ranges_path}")
        print("Using empty text ranges. Run datamodel.py to generate text_ranges.json")
        return {}
    
    try:
        with open(text_ranges_path, 'r') as f:
            data = json.load(f)
        
        # Get ranges for the specified schema
        schema_ranges = data.get('text_feature_ranges_by_schema', {}).get(schema_type, {})
        
        # Convert lists back to tuples
        text_ranges = {k: tuple(v) for k, v in schema_ranges.items()}
        
        # Get tensor dimension for this schema
        tensor_dim = data.get('tensor_dims_by_schema', {}).get(schema_type, 0)
        
        print(f"Loaded text feature ranges for schema '{schema_type}' from {text_ranges_path}")
        print(f"  Tensor dimension: {tensor_dim}")
        print(f"  Text ranges: {text_ranges}")
        
        return text_ranges
    except Exception as e:
        print(f"Error loading text ranges from {text_ranges_path}: {e}")
        print("Using empty text ranges.")
        return {}


def load_dimensions(data_dir: str = None) -> dict:
    """
    Load dimension metadata from text_ranges.json.
    
    Args:
        data_dir: Directory containing text_ranges.json. If None, uses default.
    
    Returns:
        Dict with 'event_dim', 'procedure_dim', and 'num_classes'
    """
    if data_dir is None:
        data_dir = Path(__file__).parent / 'data'
    else:
        data_dir = Path(data_dir)
    
    text_ranges_path = data_dir / 'text_ranges.json'
    
    if not text_ranges_path.exists():
        raise FileNotFoundError(
            f"text_ranges.json not found at {text_ranges_path}. "
            f"Run 'python detect_dimensions.py' to generate it."
        )
    
    with open(text_ranges_path, 'r') as f:
        data = json.load(f)
    
    if 'dimensions' not in data:
        raise ValueError(
            f"text_ranges.json missing 'dimensions' field. "
            f"Run 'python detect_dimensions.py' to add dimension metadata."
        )
    
    return data['dimensions']


def compute_masked_loss(output, targets, padding_mask, criterion):
    """
    Compute loss only on non-padded positions.
    
    Args:
        output: Model predictions (batch_size, seq_len, num_classes)
        targets: Ground truth labels (batch_size, seq_len, num_classes)
        padding_mask: Padding mask (batch_size, seq_len), True = padding
        criterion: Loss function
        
    Returns:
        Scalar loss value
    """
    # Create mask for valid positions
    valid_mask = ~padding_mask  # (batch_size, seq_len), True = valid
    
    # Compute loss per timestep
    loss_per_timestep = criterion(output, targets)  # (batch_size, seq_len, num_classes)
    
    # Average over classes
    loss_per_timestep = loss_per_timestep.mean(dim=-1)  # (batch_size, seq_len)
    
    # Mask out padded positions
    loss_per_timestep = loss_per_timestep * valid_mask.float()
    
    # Compute mean over valid timesteps
    num_valid = valid_mask.sum()
    loss = loss_per_timestep.sum() / num_valid if num_valid > 0 else loss_per_timestep.sum()
    
    return loss


def apply_random_missingness(batch_events: torch.Tensor, text_feature_ranges: dict,
                            missingness_prob: float = 0.5, 
                            min_missing: int = 5, 
                            max_missing: int = 15):
    """
    Apply random feature missingness for data augmentation.
    
    Creates a feature mask where randomly selected features are set to 0 (missing)
    in the last timestep of each sequence. For text features, masks the entire
    extended embedding range instead of individual indices.
    
    Args:
        batch_events: Event tensor of shape (batch_size, seq_len, num_features)
        text_feature_ranges: Dict mapping feature names to (start_idx, end_idx) tuples
        missingness_prob: Probability of applying missingness per sample (default: 0.5)
        min_missing: Minimum number of features to mask (default: 5)
        max_missing: Maximum number of features to mask (default: 15)
        
    Returns:
        Feature mask of shape (batch_size, seq_len, num_features) where:
        - 1.0 = feature available
        - 0.0 = feature missing (only in last timestep)
        Or None if missingness is not applied to any sample
    """
    batch_size, seq_len, num_features = batch_events.shape
    
    # Start with all features available
    feature_mask = torch.ones_like(batch_events)
    any_masked = False
    
    # Get text feature ranges for smart sampling
    text_range_positions = set()
    text_start_indices = []
    for start_idx, end_idx in text_feature_ranges.values():
        text_start_indices.append(start_idx)
        text_range_positions.update(range(start_idx, end_idx))
    
    # Get non-text feature indices
    non_text_features = [i for i in range(num_features) if i not in text_range_positions]
    
    # Available features to sample from: non-text features + text feature start indices
    available_features = non_text_features + text_start_indices
    
    # For each sample in batch, randomly apply missingness
    for i in range(batch_size):
        if random.random() < missingness_prob:
            any_masked = True
            
            # Randomly select number of features to mask
            num_missing = random.randint(min_missing, min(max_missing, len(available_features)))
            
            # Randomly sample features
            sampled_indices = random.sample(available_features, num_missing)
            
            # Apply masking - text features mask entire ranges
            for feat_idx in sampled_indices:
                # Check if this is a text feature start
                is_text = False
                for start_idx, end_idx in text_feature_ranges.values():
                    if feat_idx == start_idx:
                        # Mask entire text range
                        feature_mask[i, -1, start_idx:end_idx] = 0.0
                        is_text = True
                        break
                
                if not is_text:
                    # Regular feature - mask single index
                    feature_mask[i, -1, feat_idx] = 0.0
    
    return feature_mask if any_masked else None


def train_epoch(model, dataloader, optimizer, criterion, device, use_missingness=True, text_feature_ranges=None):
    """
    Train for one epoch.
    
    Args:
        model: EventTransformerClassifier
        dataloader: Training data loader
        optimizer: Optimizer
        criterion: Loss function
        device: Device to train on
        use_missingness: Whether to apply random feature missingness
        text_feature_ranges: Dict mapping text feature names to (start_idx, end_idx) tuples
        
    Returns:
        Average training loss
    """
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    if text_feature_ranges is None:
        text_feature_ranges = {}
    
    pbar = tqdm(dataloader, desc="Training")
    for batch in pbar:
        # Move data to device
        events = batch['events'].to(device)
        targets = batch['targets'].to(device)
        padding_mask = batch['event_padding_mask'].to(device)
        
        # Use cumulative procedures from last timestep
        last_timestep_idx = events.shape[1] - 1
        procedures_full = batch['procedures_per_event'][last_timestep_idx].to(device)
        proc_mask_full = batch['procedure_masks_per_event'][last_timestep_idx].to(device)
        
        # Apply random missingness for data augmentation
        feature_mask = None
        if use_missingness:
            feature_mask = apply_random_missingness(events, text_feature_ranges)
            if feature_mask is not None:
                feature_mask = feature_mask.to(device)
        
        # Forward pass
        output = model(
            events,
            procedure_x=procedures_full,
            src_key_padding_mask=padding_mask,
            procedure_padding_mask=proc_mask_full,
            feature_mask=feature_mask
        )
        
        # Compute loss
        loss = compute_masked_loss(output, targets, padding_mask, criterion)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # Update metrics
        total_loss += loss.item()
        num_batches += 1
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / num_batches


def validate(model, dataloader, criterion, device, text_feature_ranges=None):
    """
    Validate the model.
    
    Args:
        model: EventTransformerClassifier
        dataloader: Validation data loader
        criterion: Loss function
        device: Device to validate on
        text_feature_ranges: Dict mapping text feature names to (start_idx, end_idx) tuples (not used in validation)
        
    Returns:
        Average validation loss
    """
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Validating")
        for batch in pbar:
            # Move data to device
            events = batch['events'].to(device)
            targets = batch['targets'].to(device)
            padding_mask = batch['event_padding_mask'].to(device)
            
            # Use cumulative procedures from last timestep
            last_timestep_idx = events.shape[1] - 1
            procedures_full = batch['procedures_per_event'][last_timestep_idx].to(device)
            proc_mask_full = batch['procedure_masks_per_event'][last_timestep_idx].to(device)
            
            # Forward pass (no missingness during validation)
            output = model(
                events,
                procedure_x=procedures_full,
                src_key_padding_mask=padding_mask,
                procedure_padding_mask=proc_mask_full
            )
            
            # Compute loss
            loss = compute_masked_loss(output, targets, padding_mask, criterion)
            
            total_loss += loss.item()
            num_batches += 1
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / num_batches


def main():
    parser = argparse.ArgumentParser(description='Train EventTransformerClassifier')
    parser.add_argument('--data_path', type=str, required=True,
                        help='Path to JSON file containing tensor data')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='Directory containing text_ranges.json (default: auto-detect from data_path)')
    parser.add_argument('--event_dim', type=int, default=None,
                        help='Event feature dimension (default: auto-detect from text_ranges.json)')
    parser.add_argument('--procedure_dim', type=int, default=None,
                        help='Procedure feature dimension (default: auto-detect from text_ranges.json)')
    parser.add_argument('--num_classes', type=int, default=None,
                        help='Number of output classes (default: auto-detect from text_ranges.json)')
    parser.add_argument('--hidden_dim', type=int, default=512,
                        help='Hidden dimension size')
    parser.add_argument('--num_heads', type=int, default=8,
                        help='Number of attention heads')
    parser.add_argument('--num_layers', type=int, default=6,
                        help='Number of transformer layers')
    parser.add_argument('--dropout', type=float, default=0.1,
                        help='Dropout rate')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size')
    parser.add_argument('--epochs', type=int, default=50,
                        help='Number of training epochs')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                        help='Weight decay')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                        help='Training data ratio')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                        help='Validation data ratio')
    parser.add_argument('--use_missingness', action='store_true',
                        help='Use random feature missingness for data augmentation')
    parser.add_argument('--save_dir', type=str, default='checkpoints',
                        help='Directory to save model checkpoints')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    
    args = parser.parse_args()
    
    # Set random seeds
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # Create save directory
    save_dir = Path(args.save_dir)
    save_dir.mkdir(exist_ok=True)
    
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    print()
    
    # Auto-detect dimensions if not provided
    if args.data_dir is None:
        args.data_dir = str(Path(args.data_path).parent)
    
    if args.event_dim is None or args.procedure_dim is None or args.num_classes is None:
        print("Auto-detecting dimensions from text_ranges.json...")
        try:
            dimensions = load_dimensions(args.data_dir)
            if args.event_dim is None:
                args.event_dim = dimensions['event_dim']
            if args.procedure_dim is None:
                args.procedure_dim = dimensions['procedure_dim']
            if args.num_classes is None:
                args.num_classes = dimensions['num_classes']
            print(f"✓ Loaded dimensions: event_dim={args.event_dim}, procedure_dim={args.procedure_dim}, num_classes={args.num_classes}")
        except (FileNotFoundError, ValueError) as e:
            print(f"Error loading dimensions: {e}")
            print("Please run 'python detect_dimensions.py' first, or specify dimensions manually.")
            return
        print()
    
    # Load and split data
    tensor_data = load_tensor_data(args.data_path)
    train_data, val_data, test_data = split_data(
        tensor_data, 
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio
    )
    print()
    
    # Create data loaders
    print("Creating data loaders...")
    train_loader = create_dataloader(
        train_data,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0
    )
    val_loader = create_dataloader(
        val_data,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0
    )
    test_loader = create_dataloader(
        test_data,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0
    )
    print(f"✓ Created data loaders: {len(train_loader)} train batches, "
          f"{len(val_loader)} val batches, {len(test_loader)} test batches")
    print()
    
    # Initialize model
    print("Initializing model...")
    model = EventTransformerClassifier(
        input_dim=args.event_dim,
        procedure_dim=args.procedure_dim,
        num_classes=args.num_classes,
        hidden_dim=args.hidden_dim,
        num_heads=args.num_heads,
        num_transformer_layers=args.num_layers,
        dropout=args.dropout
    ).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"✓ Model initialized")
    print(f"  Total parameters: {num_params:,}")
    print(f"  Trainable parameters: {num_trainable:,}")
    print()
    
    # Get text feature ranges (loads from text_ranges.json)
    data_dir = Path(args.data_path).parent
    text_feature_ranges = get_text_feature_ranges(str(data_dir), schema_type='event')
    if text_feature_ranges:
        print(f"Text feature ranges loaded for 'event' schema:")
        for name, (start, end) in text_feature_ranges.items():
            print(f"  {name}: indices [{start}, {end})")
    else:
        print("No text feature ranges loaded (all features treated as independent)")
    print()

    
    # Initialize optimizer and scheduler
    optimizer = AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5
    )
    
    # Loss function (BCEWithLogitsLoss for multi-label classification)
    criterion = nn.BCEWithLogitsLoss(reduction='none')
    
    # Training loop
    print("=" * 60)
    print("Starting training...")
    print("=" * 60)
    print()
    
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 10
    
    for epoch in range(args.epochs):
        print(f"Epoch {epoch + 1}/{args.epochs}")
        print("-" * 60)
        
        # Train
        train_loss = train_epoch(
            model, train_loader, optimizer, criterion, device,
            use_missingness=args.use_missingness,
            text_feature_ranges=text_feature_ranges
        )
        
        # Validate
        val_loss = validate(model, val_loader, criterion, device, text_feature_ranges)
        
        # Update scheduler
        scheduler.step(val_loss)
        
        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        print()
        
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            checkpoint_path = save_dir / 'best_model.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'args': vars(args)
            }, checkpoint_path)
            print(f"✓ Saved best model to {checkpoint_path}")
            print()
        else:
            patience_counter += 1
            if patience_counter >= max_patience:
                print(f"Early stopping triggered after {max_patience} epochs without improvement")
                break
    
    # Load best model and evaluate on test set
    print("=" * 60)
    print("Evaluating best model on test set...")
    print("=" * 60)
    print()
    
    checkpoint = torch.load(save_dir / 'best_model.pt')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    test_loss = validate(model, test_loader, criterion, device)
    print(f"Test Loss: {test_loss:.4f}")
    print()
    
    print("=" * 60)
    print("Training complete!")
    print("=" * 60)
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Test loss: {test_loss:.4f}")
    print(f"Model saved to: {save_dir / 'best_model.pt'}")


if __name__ == '__main__':
    main()
