"""
Process JSON mapping to generate Category classes and fieldMap.

Usage:
    python proc_json.py [mapping_file] [output_dir]
    
Examples:
    python proc_json.py  # Uses ../data/mapping.json, outputs to generated_categories/
    python proc_json.py ../data/mapping.json generated_categories
"""

import json
import torch
import sys
from pathlib import Path
from typing import Dict
from abstract import DataPoint, Category
from sentence_transformers import SentenceTransformer
from generate_category import generate_category_class


def sanitize_class_name(field_name: str) -> str:
    """
    Convert a field name to a valid Python class name.
    
    Examples:
        'PRIMARY_DIAGNOSIS' -> 'PrimaryDiagnosisCategory'
        'SEX' -> 'SexCategory'
        'VIZ_MSDRG_CODE' -> 'VizMsdrgCodeCategory'
    """
    # Split by underscore and capitalize each part
    parts = field_name.lower().split('_')
    class_name = ''.join(word.capitalize() for word in parts)
    
    # Add 'Category' suffix if not already present
    if not class_name.endswith('Category'):
        class_name += 'Category'
    
    return class_name


def generate_fieldmap_from_mapping(mapping_file: str, output_dir: str = "generated_categories"):
    """
    Read mapping.json and generate Category classes for each categorical field.
    
    Args:
        mapping_file: Path to mapping.json
        output_dir: Directory to store generated category classes
    
    Returns:
        Tuple of (generated_classes dict, fieldmap dict)
    """
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Create __init__.py to make it a package
    init_file = output_path / "__init__.py"
    init_file.touch(exist_ok=True)
    
    # Read mapping.json
    with open(mapping_file, 'r') as f:
        mapping = json.load(f)
    
    # Track generated classes
    generated_classes = {}
    fieldmap = {}
    
    # Type mapping for non-Category types
    type_map = {
        'int': int,
        'float': float,
        'str': str,
        'bool': bool,
    }
    
    print(f"Processing {len(mapping)} fields from {mapping_file}...")
    print()
    
    # Process each field
    for field_name, field_type in mapping.items():
        if field_type == 'Category':
            # Generate unique class name
            class_name = sanitize_class_name(field_name)
            
            # Generate the class file
            output_file = output_path / f"{class_name.lower()}.py"
            generate_category_class(class_name, str(output_file))
            
            # Store in generated classes dict
            generated_classes[field_name] = class_name
            
            # Add to fieldmap (will be imported later)
            fieldmap[field_name] = class_name
            
        elif field_type in type_map:
            # Built-in type
            fieldmap[field_name] = type_map[field_type].__name__
        else:
            # Custom type (like 'EventDataPoint')
            fieldmap[field_name] = field_type
    
    print()
    print(f"✓ Generated {len(generated_classes)} Category classes in {output_dir}/")
    print()
    
    # Generate the imports and fieldMap code
    generate_fieldmap_code(generated_classes, fieldmap, output_dir)
    
    return generated_classes, fieldmap


def generate_fieldmap_code(generated_classes: Dict[str, str], fieldmap: Dict, output_dir: str):
    """
    Generate Python code with imports and fieldMap definition.
    """
    
    # Build imports
    import_lines = []
    for field_name, class_name in sorted(generated_classes.items()):
        module_name = class_name.lower()
        import_lines.append(f"from {output_dir}.{module_name} import {class_name}")
    
    # Build fieldMap dictionary
    fieldmap_lines = ["fieldMap = {"]
    for field_name, field_type in sorted(fieldmap.items()):
        if isinstance(field_type, str) and field_type in generated_classes.values():
            # Category class (no quotes)
            fieldmap_lines.append(f"    '{field_name}': {field_type},")
        elif field_type in ['int', 'float', 'str', 'bool']:
            # Built-in type (no quotes)
            fieldmap_lines.append(f"    '{field_name}': {field_type},")
        else:
            # String literal (with quotes)
            fieldmap_lines.append(f"    '{field_name}': '{field_type}',")
    fieldmap_lines.append("}")
    
    # Write to output file
    output_file = Path(output_dir) / "fieldmap_generated.py"
    with open(output_file, 'w') as f:
        f.write('"""\n')
        f.write('Auto-generated fieldMap with Category classes.\n')
        f.write('Generated by proc_json.py\n')
        f.write('"""\n\n')
        
        # Write imports
        f.write('\n'.join(import_lines))
        f.write('\n\n')
        
        # Write fieldMap
        f.write('\n'.join(fieldmap_lines))
        f.write('\n')
    
    print(f"✓ Generated fieldMap code in {output_file}")
    print()
    print("=" * 60)
    print("To use in your code:")
    print("=" * 60)
    print(f"from {output_dir}.fieldmap_generated import fieldMap")
    print()
    print("Or copy the imports and fieldMap into your datamodel.py")
    print("=" * 60)


class Event:
    # Class-level shared model instance
    _model = None
    
    # Class-level text feature ranges tracking per schema type
    # Maps schema_type -> {field_name -> (start_idx, end_idx)}
    _text_feature_ranges_by_schema = {}
    _tensor_dims_by_schema = {}
    
    @classmethod
    def _get_model(cls):
        """Lazy initialization of the shared sentence transformer model."""
        if cls._model is None:
            cls._model = SentenceTransformer('all-MiniLM-L6-v2')
        return cls._model
    
    @classmethod
    def get_text_feature_ranges(cls, schema_type: str = None) -> dict:
        """
        Get the text feature ranges detected during tensor conversion.
        
        Args:
            schema_type: The schema type (e.g., 'event', 'procedure'). 
                        If None, returns all schemas.
        
        Returns:
            Dict mapping field names to (start_idx, end_idx) tuples.
            Example: {'diagnosis': (50, 70), 'procedure_desc': (70, 90)}
        """
        if schema_type:
            return cls._text_feature_ranges_by_schema.get(schema_type, {}).copy()
        return cls._text_feature_ranges_by_schema.copy()
    
    @classmethod
    def export_text_ranges(cls, output_path: str):
        """
        Export text feature ranges to a JSON file for use in training.
        Saves separate ranges for each schema type.
        
        Args:
            output_path: Path to save the text_ranges.json file
        """
        import json
        with open(output_path, 'w') as f:
            json.dump({
                'text_feature_ranges_by_schema': cls._text_feature_ranges_by_schema,
                'tensor_dims_by_schema': cls._tensor_dims_by_schema
            }, f, indent=2)
        print(f"✓ Exported text feature ranges to {output_path}")
    
    def __init__(self, data: DataPoint, field_names: list = None, schema_type: str = 'event', use_llm: bool = True):
        """
        Initialize Event from DataPoint.
        
        Args:
            data: DataPoint containing the event data
            field_names: Optional list of field names corresponding to data.dump_contents().
                        Used to track which fields are text embeddings.
            schema_type: Type of schema ('event', 'procedure', etc.) for tracking ranges separately
            use_llm: Whether to use LLM embeddings for text (default: True)
        """
        # Get the shared model instance
        model = self._get_model()
        
        self.parsed_vec = []
        self.schema_type = schema_type
        current_idx = 0
        
        # Initialize tracking for this schema type if not exists
        if schema_type not in Event._text_feature_ranges_by_schema:
            Event._text_feature_ranges_by_schema[schema_type] = {}
        
        # Get field contents and names
        contents = data.dump_contents()
        
        # If field_names not provided, try to get from data
        if field_names is None and hasattr(data, '__dict__'):
            # Try to infer field names from the data structure
            field_names = [None] * len(contents)
        
        for i, info in enumerate(contents):
            field_name = field_names[i] if field_names and i < len(field_names) else None
            
            if isinstance(info, Category):
                self.parsed_vec.append(float(info.get_cat()))
                current_idx += 1
            elif type(info) == int or type(info) == bool:
                self.parsed_vec.append(float(info))
                current_idx += 1
            elif type(info) == str:
                # Convert string to embedding and extend the list
                start_idx = current_idx
                embedding = model.encode(info)
                self.parsed_vec.extend(embedding.tolist())
                end_idx = current_idx + len(embedding)
                current_idx = end_idx
                
                # Track this as a text feature range for this schema type
                if field_name:
                    Event._text_feature_ranges_by_schema[schema_type][field_name] = (start_idx, end_idx)
        
        # Update the tensor dimension for this schema type
        Event._tensor_dims_by_schema[schema_type] = len(self.parsed_vec)
    
    def to_tensor(self) -> torch.Tensor:
        """Convert parsed_vec to a PyTorch tensor."""
        return torch.tensor(self.parsed_vec, dtype=torch.float32)


def main():
    """Main entry point for generating fieldMap from mapping.json."""
    if len(sys.argv) < 2:
        # Default to mapping.json in parent data directory
        mapping_file = Path(__file__).parent.parent / "data" / "mapping.json"
    else:
        mapping_file = sys.argv[1]
    
    if not Path(mapping_file).exists():
        print(f"Error: {mapping_file} not found")
        sys.exit(1)
    
    output_dir = sys.argv[2] if len(sys.argv) >= 3 else "generated_categories"
    
    print("=" * 60)
    print("Generating fieldMap from mapping.json")
    print("=" * 60)
    print()
    
    generate_fieldmap_from_mapping(str(mapping_file), output_dir)
    
    print()
    print("✓ Done!")


if __name__ == "__main__":
    main()
